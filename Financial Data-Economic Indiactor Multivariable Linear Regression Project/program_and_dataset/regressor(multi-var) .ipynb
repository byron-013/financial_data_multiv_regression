{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import os\n",
    "import shutil\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy.stats import probplot, shapiro, boxcox\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "#CHANGE PATH TO YOUR PATH TO diagnostic_results FOLDER in the read_and_recommend function below!!! The diagnostic_results folder will be created in the current folder you are in.\n",
    "\n",
    "\n",
    "def read_and_recommend(input_file_name, summary_file_name):\n",
    "    \"\"\"\n",
    "    Reads the summary of the regression model from the input file and provides recommendations in the summary file.\n",
    "    \"\"\"\n",
    "    ######### # Define the directory path\n",
    "    ######### dir_path = r'C:\\YOUR PATH PORTION HERE\\diagnostic_results'\n",
    "\n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(dir_path):\n",
    "        # If not, create the directory\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "    # Define the full path for the input and summary files\n",
    "    input_file_path = os.path.join(dir_path, input_file_name)\n",
    "    summary_file_path = os.path.join(dir_path, summary_file_name)\n",
    "\n",
    "    # Check if the input file exists\n",
    "    if not os.path.exists(input_file_path):\n",
    "        print(f\"Input file {input_file_path} does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Check if the summary file exists\n",
    "    if not os.path.exists(summary_file_path):\n",
    "        # If not, create an empty file\n",
    "        open(summary_file_path, 'w').close()\n",
    "\n",
    "    # Open the input file and read the summary\n",
    "    with open(input_file_path, 'r') as f:\n",
    "        summary = f.read()\n",
    "\n",
    "    # Open the summary file in append mode\n",
    "    with open(summary_file_path, 'a') as f:\n",
    "\n",
    "        # Provide recommendations based on the summary\n",
    "        if \"Prob (F-statistic):\" in summary:\n",
    "            recommendation = \"Recommendation: If the F-statistic is not significant, consider applying a logarithmic transformation to your dependent variable. This can help with non-linearity and heteroscedasticity.\"\n",
    "            print(recommendation)\n",
    "            f.write(recommendation + '\\n')\n",
    "        if \"Durbin-Watson:\" in summary:\n",
    "            recommendation = \"Recommendation: If the Durbin-Watson statistic indicates autocorrelation, consider applying a square root transformation to your independent variables. This can help stabilize variance.\"\n",
    "            print(recommendation)\n",
    "            f.write(recommendation + '\\n')\n",
    "        if \"Jarque-Bera (JB):\" in summary:\n",
    "            recommendation = \"Recommendation: If the Jarque-Bera test indicates non-normality, consider applying a square transformation to your dependent variable. This can help achieve normality.\"\n",
    "            print(recommendation)\n",
    "            f.write(recommendation + '\\n')\n",
    "        if \"Skew:\" in summary:\n",
    "            recommendation = \"Recommendation: If the data is skewed, consider applying a cube root or Yeo-Johnson transformation. These can help reduce skewness.\"\n",
    "            print(recommendation)\n",
    "            f.write(recommendation + '\\n')\n",
    "        if \"Cond. No.\" in summary:\n",
    "            recommendation = \"Recommendation: If the condition number is high, indicating potential multicollinearity, consider centering your variables by subtracting the mean or standardizing them. This can help mitigate multicollinearity.\"\n",
    "            print(recommendation)\n",
    "            f.write(recommendation + '\\n')\n",
    "        if \"Omnibus:\" in summary:\n",
    "            recommendation = \"Recommendation: If the Omnibus test indicates non-normality, consider applying a Box-Cox transformation. This can help achieve normality.\"\n",
    "            print(recommendation)\n",
    "            f.write(recommendation + '\\n')\n",
    "\n",
    "def get_stock_data(ticker, start_date, end_date, fetch_range_start=0, fetch_range_end=0):\n",
    "    \"\"\"\n",
    "    Fetches stock data for a given ticker and date range from yfinance.\n",
    "    \"\"\"\n",
    "    # Extend the date range for fetching data to ensure calculations go further back\n",
    "    extended_start_date = pd.Timestamp(start_date) - pd.DateOffset(days=fetch_range_start)\n",
    "    extended_end_date = pd.Timestamp(end_date) + pd.DateOffset(days=fetch_range_end)\n",
    "    \n",
    "    stock = yf.Ticker(ticker)\n",
    "    data = stock.history(start=extended_start_date, end=extended_end_date)\n",
    "    \n",
    "    # Convert the datetime index to date\n",
    "    data.index = data.index.date\n",
    "    \n",
    "    return data\n",
    "\n",
    "def move_csv_files(folder='.'):\n",
    "    # Create DATA directory if it doesn't exist\n",
    "    if not os.path.exists('DATA'):\n",
    "        os.makedirs('DATA')\n",
    "\n",
    "    # Iterate over all files in the specified folder\n",
    "    for filename in os.listdir(folder):\n",
    "        # Check if the file is a .csv file\n",
    "        if filename.endswith('.csv'):\n",
    "            # Construct full file path\n",
    "            source = os.path.join(folder, filename)\n",
    "            destination = os.path.join('DATA', filename)\n",
    "            # Move the file to the DATA directory\n",
    "            shutil.move(source, destination)\n",
    "    \n",
    "def calculate_moving_average(data, period):\n",
    "    \"\"\"\n",
    "    Calculates the moving average of a given period.\n",
    "    \"\"\"\n",
    "    return data['Close'].rolling(window=period).mean()\n",
    "\n",
    "def calculate_bollinger_bands(data, period, num_std):\n",
    "    \"\"\"\n",
    "    Calculates the daily upper and lower Bollinger Bands.\n",
    "    \"\"\"\n",
    "    rolling_mean = data['Close'].rolling(window=period).mean()\n",
    "    rolling_std = data['Close'].rolling(window=period).std()\n",
    "    upper_band = rolling_mean + (rolling_std * num_std)\n",
    "    lower_band = rolling_mean - (rolling_std * num_std)\n",
    "    return upper_band, lower_band\n",
    "\n",
    "def calculate_rsi(data, period):\n",
    "    \"\"\"\n",
    "    Calculates the daily Relative Strength Index (RSI).\n",
    "    \"\"\"\n",
    "    delta = data['Close'].diff(1)\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    avg_gain = gain.rolling(window=period, min_periods=1).mean()\n",
    "    avg_loss = loss.rolling(window=period, min_periods=1).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def calculate_annual_dividend_yield(data, ticker):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculates the annual Dividend Yield based on the sum of the last 12 months of dividends and the current stock price.\n",
    "    \"\"\"\n",
    "    stock = yf.Ticker(ticker)\n",
    "    info = stock.info\n",
    "    annual_dividends = info.get('dividendRate', 0)\n",
    "    current_price = data['Close'].iloc[-1]\n",
    "    dividend_yield = annual_dividends / current_price if current_price else 0\n",
    "    return dividend_yield\n",
    "\n",
    "def run_diagnostics_part1(data, independent_vars, dependent_var):\n",
    "    \"\"\"\n",
    "    Runs the first set of diagnostics on the data and returns the results.\n",
    "    \"\"\"\n",
    "    # Calculate VIF for each independent variable\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"variables\"] = independent_vars\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(data[independent_vars].values, i) for i in range(len(independent_vars))]\n",
    "\n",
    "    # Boxplot for each variable\n",
    "    for var in independent_vars:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(x=data[var])\n",
    "        plt.title(f'Boxplot of {var}')\n",
    "        plt.savefig(f'diagnostic_results/boxplot_{var}.png')\n",
    "\n",
    "    # Scatterplot for each variable\n",
    "    for var in independent_vars:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(data[var], data[dependent_var])\n",
    "        plt.xlabel(var)\n",
    "        plt.ylabel(dependent_var)\n",
    "        plt.title(f'Scatterplot of {dependent_var} vs {var}')\n",
    "        plt.savefig(f'diagnostic_results/scatterplot_{var}.png')\n",
    "\n",
    "    return vif\n",
    "\n",
    "def run_diagnostics_part2(file_name, output_file_name):\n",
    "    \"\"\"\n",
    "    Runs the second set of diagnostics on the regression model and saves the results to a text file.\n",
    "    \"\"\"\n",
    "    # Load the data from the CSV file\n",
    "    data= pd.read_csv(file_name)\n",
    "    \n",
    "    # Define the dependent variable (column 2) and independent variables (the rest of the columns)\n",
    "    dependent_var = data[data.columns[1]]\n",
    "    independent_vars = data[data.columns[2:]]\n",
    "    noConstant_indep_vars = data[data.columns[2:-1]]\n",
    "    import warnings\n",
    "    from statsmodels.tools.sm_exceptions import ValueWarning\n",
    "    warnings.filterwarnings(action='ignore', category=ValueWarning)\n",
    "\n",
    "    # Run regression on the data and save summary\n",
    "    model = sm.OLS(dependent_var, noConstant_indep_vars).fit()\n",
    "\n",
    "    # Calculate residuals\n",
    "    residuals = model.resid\n",
    "\n",
    "    # Breusch-Pagan test\n",
    "    bp_test = het_breuschpagan(residuals, independent_vars)\n",
    "\n",
    "    # If the p-value of the Breusch-Pagan test is less than 0.05, we reject the null hypothesis of homoscedasticity, i.e., we have heteroscedasticity.\n",
    "    if bp_test[1] < 0.05:\n",
    "        print(\"The p-value of the Breusch-Pagan test is less than 0.05, indicating heteroscedasticity. Consider using robust standard errors.\")\n",
    "        model = model.get_robustcov_results()\n",
    "    else:\n",
    "        print(\"The p-value of the Breusch-Pagan test is not less than 0.05, indicating no heteroscedasticity. Non-robust standard errors can be used.\")\n",
    "        \n",
    "    # Calculate residuals\n",
    "    residuals = model.resid\n",
    "\n",
    "\n",
    "    # Plot the residuals\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(model.fittedvalues, model.resid)\n",
    "    plt.xlabel('Fitted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residuals vs Fitted Values')\n",
    "    \n",
    "\n",
    "    # Durbin-Watson test\n",
    "    dw = durbin_watson(residuals)\n",
    "\n",
    "    # Shapiro-Wilk test\n",
    "    shapiro_test = shapiro(residuals)\n",
    "\n",
    "    # Prepare independent variables for the tests\n",
    "    exog = independent_vars\n",
    "    exog3 = noConstant_indep_vars\n",
    "\n",
    "    # Calculate the rank of the exog matrix\n",
    "    rank_exog = np.linalg.matrix_rank(exog) \n",
    "\n",
    "    # Calculate model.df_model + 1\n",
    "    df_model_plus_one = model.df_model+1\n",
    "    \n",
    "    # Compare the two values\n",
    "   \n",
    "    # Breusch-Pagan test\n",
    "    bp_test = het_breuschpagan(residuals, exog)\n",
    "\n",
    "\n",
    "    # Q-Q plot of the residuals\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    probplot(residuals, dist=\"norm\", plot=plt)\n",
    "    plt.title(\"Normal Q-Q plot\")\n",
    "    plt.savefig('diagnostic_results/qq_plot.png')\n",
    "\n",
    "    # Save the summary to a text file\n",
    "    with open(os.path.join('diagnostic_results', 'diagnostics_summary_part_2.txt'), 'w') as f:\n",
    "        f.write(model.summary().as_text())\n",
    "\n",
    "    # Return the diagnostics\n",
    "    return dw, shapiro_test, bp_test\n",
    "\n",
    "def plot_regressions(model_train, model_test):\n",
    "    # Create a new figure\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot the train data\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(model_train.fittedvalues, model_train.model.endog, label='Data')\n",
    "    plt.plot(model_train.fittedvalues, model_train.fittedvalues, color='red', label='Fit')\n",
    "    plt.xlabel('Fitted Values')\n",
    "    plt.ylabel('Actual Values')\n",
    "    plt.title('Train Data')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot the test data\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(model_test.fittedvalues, model_test.model.endog, label='Data')\n",
    "    plt.plot(model_test.fittedvalues, model_test.fittedvalues, color='red', label='Fit')\n",
    "    plt.xlabel('Fitted Values')\n",
    "    plt.ylabel('Actual Values')\n",
    "    plt.title('Test Data')\n",
    "    plt.legend()\n",
    "\n",
    "    # Adjust the layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "# Define transformation functions\n",
    "def log_transform(data, column):\n",
    "    data[column] = np.log(data[column])\n",
    "def sqrt_transform(data, column):\n",
    "    data[column] = np.sqrt(data[column])\n",
    "def cube_root_transform(data, column):\n",
    "    data[column] = np.cbrt(data[column])\n",
    "def polynomial_transform(data, column, degree):\n",
    "    data[column] = data[column] ** degree\n",
    "def standardize(data, column):\n",
    "    scaler = StandardScaler()\n",
    "    data[column] = scaler.fit_transform(data[[column]])\n",
    "def center(data, column):\n",
    "    mean = data[column].mean()\n",
    "    data[column] = data[column] - mean\n",
    "def box_cox_transform(data, column):\n",
    "    data[column], _ = stats.boxcox(data[column])\n",
    "def yeo_johnson_transform(data, column):\n",
    "    pt = PowerTransformer(method='yeo-johnson')\n",
    "    data[column] = pt.fit_transform(data[[column]])\n",
    "\n",
    "\n",
    "# Define the main function\n",
    "def transform_and_run_regression(train_file, test_file, transformations, error_type):\n",
    "    # Load the data\n",
    "    train_data = pd.read_csv(train_file)\n",
    "    test_data = pd.read_csv(test_file)\n",
    "\n",
    "    # Apply transformations\n",
    "    for column, transformation in transformations:\n",
    "        if transformation == 'log':\n",
    "            log_transform(train_data, column)\n",
    "            log_transform(test_data, column)\n",
    "        elif transformation == 'sqrt':\n",
    "            sqrt_transform(train_data, column)\n",
    "            sqrt_transform(test_data, column)\n",
    "        elif transformation == 'cube_root':\n",
    "            cube_root_transform(train_data, column)\n",
    "            cube_root_transform(test_data, column)\n",
    "        elif transformation == 'polynomial':\n",
    "            polynomial_transform(train_data, column, 2)  # Change the degree as needed\n",
    "            polynomial_transform(test_data, column, 2)\n",
    "        elif transformation == 'standardize':\n",
    "            standardize(train_data, column)\n",
    "            standardize(test_data, column)\n",
    "        elif transformation == 'center':\n",
    "            center(train_data, column)\n",
    "            center(test_data, column)\n",
    "        elif transformation == 'box_cox':\n",
    "            box_cox_transform(train_data, column)\n",
    "            box_cox_transform(test_data, column)\n",
    "        elif transformation == 'yeo_johnson':\n",
    "            yeo_johnson_transform(train_data, column)\n",
    "            yeo_johnson_transform(test_data, column)\n",
    "\n",
    "    # Define the dependent variable (column 2) and independent variables (the rest of the columns)\n",
    "    dependent_var_train = train_data[train_data.columns[1]]\n",
    "    independent_vars_train = train_data[train_data.columns[2:]]\n",
    "\n",
    "    dependent_var_test = test_data[test_data.columns[1]]\n",
    "    independent_vars_test = test_data[test_data.columns[2:]]\n",
    "\n",
    "    # Run regression on the train data and save summary\n",
    "    model_train = sm.OLS(dependent_var_train, independent_vars_train).fit(cov_type=error_type)\n",
    "\n",
    "    # Run regression on the test data and save summary\n",
    "    model_test = sm.OLS(dependent_var_test, independent_vars_test).fit(cov_type=error_type)\n",
    "\n",
    "    # Create a new directory for the final results\n",
    "    if not os.path.exists('Final_Results'):\n",
    "        os.makedirs('Final_Results')\n",
    "\n",
    "    # Save the summaries to a text file in the new directory\n",
    "    with open(os.path.join('Final_Results', 'train_summary.txt'), 'w') as f:\n",
    "        f.write(model_train.summary().as_text())\n",
    "    with open(os.path.join('Final_Results', 'test_summary.txt'), 'w') as f:\n",
    "        f.write(model_test.summary().as_text())\n",
    "\n",
    "    # Plot both regressions with best fit line and save them both to the Final Results folder\n",
    "    plot_regressions(model_train, model_test)\n",
    "\n",
    "    # Save the plots to the new directory\n",
    "    plt.savefig(os.path.join('Final_Results', 'final_regression_plots.png'))\n",
    "\n",
    "    print(\"The summaries have been saved to the 'Final_Results' directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "ticker = 'AAPL'  # Or any other ticker you want to analyze\n",
    "start_date = '2013-01-01'\n",
    "end_date = '2022-01-01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE REGRESSION VARIABLE MENU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Fetch stock data with an extended buffer to ensure enough data points for calculations\n",
    "data = get_stock_data(ticker, start_date, end_date, fetch_range_start=365 + 20, fetch_range_end=0)\n",
    "\n",
    "# Calculate indicators\n",
    "data['MA20'] = calculate_moving_average(data, 20)\n",
    "data['RSI14'] = calculate_rsi(data, 14)\n",
    "upper_band, lower_band = calculate_bollinger_bands(data, 20, 2)\n",
    "data['Bollinger_Upper'] = upper_band\n",
    "data['Bollinger_Lower'] = lower_band\n",
    "data['Annual_Dividend_Yield'] = calculate_annual_dividend_yield(data, ticker)\n",
    "\n",
    "# Reset index and rename the column to 'Date'\n",
    "data.reset_index(inplace=True)\n",
    "data.rename(columns={'index': 'Date'}, inplace=True)\n",
    "\n",
    "# Convert start_date and end_date to datetime.date\n",
    "start_date = pd.to_datetime(start_date).date()\n",
    "end_date = pd.to_datetime(end_date).date()\n",
    "\n",
    "# Filter data within the specified date range\n",
    "data_within_range = data[(data['Date'] >= start_date) & (data['Date'] <= end_date)]\n",
    "\n",
    "# Save all data to a CSV file\n",
    "data_within_range.to_csv('calculated.vars.csv', index=False)\n",
    "\n",
    "# Load all data from the CSV file\n",
    "all_data = pd.read_csv('calculated.vars.csv')\n",
    "\n",
    "# Convert 'Date' column to datetime.date\n",
    "all_data['Date'] = pd.to_datetime(all_data['Date']).dt.date\n",
    "\n",
    "# Filter data within the specified date range\n",
    "data_within_range = all_data[(all_data['Date'] >= start_date) & (all_data['Date'] <= end_date)]\n",
    "\n",
    "# Save all data to a CSV file\n",
    "data_within_range.to_csv('calculated.vars.csv', index=False)\n",
    "\n",
    "# Define the file names\n",
    "calculated_vars_file = 'calculated.vars.csv'\n",
    "economic_data_file = 'economic.data.csv'\n",
    "\n",
    "# Load the data and convert the 'Date' column to datetime\n",
    "calculated_vars = pd.read_csv(calculated_vars_file)\n",
    "calculated_vars['Date'] = pd.to_datetime(calculated_vars['Date']).dt.date\n",
    "\n",
    "economic_data = pd.read_csv(economic_data_file)\n",
    "economic_data['Date'] = pd.to_datetime(economic_data['Date']).dt.date\n",
    "\n",
    "# Merge the dataframes on the 'Date' column\n",
    "merged_data = pd.merge(calculated_vars, economic_data, on='Date', how='left')\n",
    "\n",
    "# Fill the missing values in each column by using the nearest entry\n",
    "merged_data.bfill(inplace=True)\n",
    "merged_data.ffill(inplace=True)\n",
    "\n",
    "# Define the output file name\n",
    "output_file = 'regression.variable.menu.csv'\n",
    "\n",
    "# Save the dataframe to a new csv file\n",
    "merged_data.to_csv(output_file, index=False)\n",
    "\n",
    "# Load the 'regression.variable.menu.csv' file\n",
    "menu_data = pd.read_csv('regression.variable.menu.csv')\n",
    "menu_data['Date'] = pd.to_datetime(menu_data['Date']).dt.date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHOOSE VARIABLES AND SPLIT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# Specify the column names you want to select for regression\n",
    "column_names_menu_selection = ['Date', 'Close', 'Volume', 'Unemployment Rate', 'rf_3 Yr']  # replace with your actual column names\n",
    "\n",
    "#Keep the Date Selected; the second choice will be used as the dependent variable\n",
    "\n",
    "# Select specified columns\n",
    "noConstant_columns_selected = menu_data[column_names_menu_selection]\n",
    "\n",
    "# Save the noConstant selected columns to a new CSV file\n",
    "noConstant_columns_selected.to_csv('noConstant_columns_selected.csv', index=False)\n",
    "\n",
    "# Load the 'noConstant_columns_selected.csv' file\n",
    "noConstant_columns_selected = pd.read_csv('noConstant_columns_selected.csv')\n",
    "\n",
    "#Add a constant column for linear regression sm.add_constant\n",
    "yesConstant_columns_selected = sm.add_constant(noConstant_columns_selected,prepend=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(yesConstant_columns_selected, test_size=0.2, shuffle=False)\n",
    "\n",
    "#Save the training and testing sets to separate CSV files\n",
    "train_data.to_csv('train_data.csv', index=False)\n",
    "test_data.to_csv('test_data.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIAGNOSTIC TEST 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "\n",
    "# Get the column names\n",
    "column_names_diag1 = train_data.columns.tolist()\n",
    "\n",
    "\n",
    "# Specify your dependent variable and independent variables\n",
    "dependent_var = column_names_diag1[1]  # set to col 1 b/c col 0 is date and when selecting columns(variables) for regression,the first non date column will be the dependent variable\n",
    "independent_vars = column_names_diag1[2:-1]  # assuming the rest of the columns are the independent variables\n",
    "print (independent_vars)\n",
    "# Run the first set of diagnostics\n",
    "\n",
    "# Create a directory for diagnostics if it doesn't exist\n",
    "if not os.path.exists('diagnostic_results'):\n",
    "    os.makedirs('diagnostic_results')\n",
    "\n",
    "# Run the first set of diagnostics\n",
    "vif = run_diagnostics_part1(train_data, independent_vars, dependent_var)\n",
    "\n",
    "# Print VIF and suggest variables to remove\n",
    "print(vif)\n",
    "high_vif_vars = vif[vif['VIF'] > 30]['variables']\n",
    "if not high_vif_vars.empty:\n",
    "    print(\"\\nConsider removing the following variables due to high multicollinearity:\")\n",
    "    print(high_vif_vars)\n",
    "else:\n",
    "    print(\"\\nNo variables to remove. All VIF values are below the threshold.\")\n",
    "\n",
    "\n",
    "# Save the summary to a text file\n",
    "with open(os.path.join('diagnostic_results', 'diagnostics_summary_part1.txt'), 'w') as f:\n",
    "    f.write(\"Variance Inflation Factor (VIF):\\n\")\n",
    "    f.write(vif.to_string())\n",
    "    if not high_vif_vars.empty:\n",
    "        f.write(\"\\n\\nConsider removing the following variables due to high multicollinearity:\\n\")\n",
    "        f.write(high_vif_vars.to_string())\n",
    "        f.write(\"\\n\\nIf these variables are important for your analysis, consider using techniques that are less sensitive to multicollinearity, such as Ridge Regression or Principal Component Analysis (PCA).\")\n",
    "    else:\n",
    "        f.write(\"\\n\\nNo variables to remove. All VIF values are below the threshold.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIAGNOSTIC TEST 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw, shapiro_test, bp_test = run_diagnostics_part2('train_data.csv', 'diagnostics_summary_part_2.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RECIEVE SUGGESTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "read_and_recommend('diagnostics_summary_part_2.txt', 'diag2_recommendations.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APPLY TRANSFORMATIONS AND RUN REGRESSION ON TEST ADN TRAIN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the transformations to apply\n",
    "# Replace 'column1', 'column2', etc. with your actual column names\n",
    "# Replace 'log', 'sqrt', etc. with the actual transformations you want to apply\n",
    "transformations = [('Unemployment Rate', 'sqrt'), ('rf_3 Yr', 'cube_root')]\n",
    "\n",
    "# Specify the type of HC standard errors to use\n",
    "error_type = 'HC3'\n",
    "\n",
    "# Specify the train and test data files\n",
    "train_file = 'train_data.csv'\n",
    "test_file = 'test_data.csv'\n",
    "\n",
    "# Call the transform_and_run_regression function\n",
    "transform_and_run_regression(train_file, test_file, transformations, error_type)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
